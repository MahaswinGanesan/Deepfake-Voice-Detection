# -*- coding: utf-8 -*-
"""1D CNN_residual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-T6sVWOiPZar4DNvtxuP7BtPRXnn-DN
"""

import tensorflow as tf

if tf.config.list_physical_devices('GPU'):
    print("✅ GPU is available.")
else:
    print("❌ GPU not available.")

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils import class_weight
from tensorflow.keras import layers, models, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from google.colab import drive

# === 1. MOUNT GOOGLE DRIVE ===
drive.mount('/content/drive')

# === 2. PATHS ===
real_folder = '/content/drive/MyDrive/extracted_real_features_500/'
fake_folder = '/content/drive/MyDrive/extracted_fake_features_500/'
real_eda_path = os.path.join(real_folder, 'Real_EDA_Report_500.xlsx')
fake_eda_path = os.path.join(fake_folder, 'Fake_EDA_Report_500.xlsx')

# === 3. LOAD FEATURES ===
def load_and_group_features(folder_path):
    audio_features = {}
    for file in os.listdir(folder_path):
        if file.endswith('.npy'):
            file_path = os.path.join(folder_path, file)
            audio_id = "_".join(file.split('_')[:3])
            if audio_id not in audio_features:
                audio_features[audio_id] = {}
            if 'mel_spec' in file:
                audio_features[audio_id]['mel_spectrogram'] = np.load(file_path)
            elif 'log_power_spec' in file:
                audio_features[audio_id]['log_spectrogram'] = np.load(file_path)
    return audio_features

real_audio_features = load_and_group_features(real_folder)
fake_audio_features = load_and_group_features(fake_folder)
audio_features = {**real_audio_features, **fake_audio_features}

# === 4. NORMALIZE FEATURES ===
def normalize_feature(feature):
    scaler = MinMaxScaler()
    return scaler.fit_transform(feature.T).T if feature.ndim > 1 else scaler.fit_transform(feature.reshape(-1, 1)).reshape(feature.shape)

def normalize_all_features(audio_features):
    normalized_data = {}
    for audio_id, features in audio_features.items():
        if all(k in features for k in ['mel_spectrogram', 'log_spectrogram']):
            normalized_data[audio_id] = {k: normalize_feature(v) for k, v in features.items()}
    return normalized_data

normalized_data = normalize_all_features(audio_features)

# === 5. COMBINE FEATURES ===
def match_sample_size(features):
    target_samples = min(map(lambda x: x.shape[0], features.values()))
    return {k: v[:target_samples] for k, v in features.items()}

def combine_features(features):
    return np.stack([features['mel_spectrogram'], features['log_spectrogram']], axis=-1)

combined = {aid: combine_features(match_sample_size(feats)) for aid, feats in normalized_data.items()}
audio_ids = list(combined.keys())

# === 6. PAD TO MAX SHAPE ===
def pad_features(features_list, max_timesteps, max_freq_bins):
    padded_features = []
    for feature in features_list:
        pad_t = max(0, max_timesteps - feature.shape[0])
        pad_f = max(0, max_freq_bins - feature.shape[1])
        padded = np.pad(feature, ((0, pad_t), (0, pad_f), (0, 0)), mode='constant')
        padded_features.append(padded)
    return np.stack(padded_features)

max_timesteps = max(f.shape[0] for f in combined.values())
max_freq_bins = max(f.shape[1] for f in combined.values())
padded_data = pad_features(list(combined.values()), max_timesteps, max_freq_bins)

# === 7. LOAD EDA LABELS ===
real_df = pd.read_excel(real_eda_path)
fake_df = pd.read_excel(fake_eda_path)

real_df['Label'] = 0
fake_df['Label'] = 1

eda_df = pd.concat([real_df, fake_df], ignore_index=True)
eda_df['Filename'] = eda_df['Filename'].str.replace('.flac', '', regex=False)

audio_id_to_label = dict(zip(eda_df['Filename'], eda_df['Label']))

# === 8. MATCH LABELS WITH DATA ===
valid_audio_ids = [aid for aid in audio_ids if aid in audio_id_to_label]
X = np.stack([padded_data[audio_ids.index(aid)] for aid in valid_audio_ids])
y = np.array([audio_id_to_label[aid] for aid in valid_audio_ids])

# === 9. DATA AUGMENTATION ===
def time_warp(signal, warp_factor=0.1):
    assert signal.ndim == 1, "time_warp expects a 1D signal"
    length = signal.shape[0]
    warp_size = int(length * warp_factor)
    src_points = np.linspace(0, length - 1, num=5)
    displacement = np.random.randint(-warp_size, warp_size + 1, size=5)
    dst_points = np.clip(src_points + displacement, 0, length - 1)
    return np.interp(np.arange(length), src_points, signal[dst_points.astype(int)])

def augment_data(X, y):
    augmented_X = []
    augmented_y = []

    for sample, label in zip(X, y):
        warped_channels = []
        for ch in range(sample.shape[-1]):
            warped = np.stack([time_warp(sample[:, f, ch]) for f in range(sample.shape[1])], axis=1)
            warped_channels.append(warped)

        warped = np.stack(warped_channels, axis=-1)
        augmented_X.append(warped)
        augmented_y.append(label)

        noisy = sample + 0.01 * np.random.normal(size=sample.shape)
        augmented_X.append(noisy)
        augmented_y.append(label)

    return np.concatenate([X, np.array(augmented_X)]), np.concatenate([y, np.array(augmented_y)])

X_augmented, y_augmented = augment_data(X, y)

# === 10. TRAIN/TEST SPLIT ===
X_train, X_test, y_train, y_test = train_test_split(
    X_augmented, y_augmented,
    test_size=0.2,
    stratify=y_augmented,
    random_state=42
)

# === 11. BUILD IMPROVED MODEL (Conv1D-compatible) ===
def residual_block(x, filters, kernel_size=3, stride=1):
    shortcut = x
    x = layers.Conv1D(filters, kernel_size, strides=stride, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.Conv1D(filters, kernel_size, padding='same')(x)
    x = layers.BatchNormalization()(x)
    if stride != 1 or shortcut.shape[-1] != filters:
        shortcut = layers.Conv1D(filters, 1, strides=stride)(shortcut)
        shortcut = layers.BatchNormalization()(shortcut)
    x = layers.add([x, shortcut])
    x = layers.ReLU()(x)
    return x

def build_improved_model(input_shape):
    inputs = Input(shape=input_shape)  # (timesteps, freq_bins, channels)

    # Reshape from 4D to 3D: (timesteps, freq_bins * channels)
    x = layers.Reshape((input_shape[0], input_shape[1] * input_shape[2]))(inputs)

    x = layers.Conv1D(128, 7, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling1D(2)(x)

    x = residual_block(x, 128)
    x = layers.Dropout(0.3)(x)

    x = residual_block(x, 256, stride=2)
    x = layers.Dropout(0.3)(x)

    x = residual_block(x, 512, stride=2)

    attention = layers.Conv1D(1, 1, activation='sigmoid')(x)
    x = layers.multiply([x, attention])

    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)

    model = models.Model(inputs, outputs)
    optimizer = Adam(learning_rate=0.0001)
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    )
    return model

model = build_improved_model(X_train.shape[1:])
model.summary()

# === 12. TRAINING ===
class_weights = class_weight.compute_class_weight(
    'balanced', classes=np.unique(y_train), y=y_train
)
class_weights = dict(enumerate(class_weights))

callbacks = [
    EarlyStopping(monitor='val_auc', patience=15, mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),
    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_auc', mode='max')
]

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=32,
    class_weight=class_weights,
    callbacks=callbacks
)

# === 13. EVALUATION ===
test_loss, test_acc, test_auc = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test AUC: {test_auc:.4f}")

y_pred = (model.predict(X_test) > 0.5).astype(int)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=["Real", "Fake"]))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Real", "Fake"], yticklabels=["Real", "Fake"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.legend()
plt.show()

# === 14. LATENT SPACE ANALYSIS ===
latent_model = models.Model(
    inputs=model.input,
    outputs=model.layers[-5].output
)
latent_features = latent_model.predict(X_test)
print(f"\nLatent features shape: {latent_features.shape}")

unique, train_counts = np.unique(y_train, return_counts=True)
unique, test_counts = np.unique(y_test, return_counts=True)

print("\nTrain label distribution:")
for label, count in zip(unique, train_counts):
    print(f"  Label {label} → {count} samples")

print("\nTest label distribution:")
for label, count in zip(unique, test_counts):
    print(f"  Label {label} → {count} samples")

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# === FLATTEN LATENT FEATURES ===
latent_features_flat = latent_features.reshape(latent_features.shape[0], -1)  # (600, 8192)

# === APPLY PCA ===
pca = PCA(n_components=2)
latent_2d = pca.fit_transform(latent_features_flat)

# === PLOT ===
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x=latent_2d[:, 0],
    y=latent_2d[:, 1],
    hue=y_test,
    palette={0: "green", 1: "red"},
    alpha=0.7
)
plt.title("PCA of Latent Features (Flattened)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Label", labels=["Real", "Fake"])
plt.grid(True)
plt.show()

unique, train_counts = np.unique(y_train, return_counts=True)
unique, test_counts = np.unique(y_test, return_counts=True)

print("\nTrain label distribution:")
for label, count in zip(unique, train_counts):
    print(f"  Label {label} → {count} samples")

print("\nTest label distribution:")
for label, count in zip(unique, test_counts):
    print(f"  Label {label} → {count} samples")

model.save('/content/drive/MyDrive/trained_model.h5')

pip install tensorflow openpyxl

# === IMPORTS ===
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils import class_weight
from tensorflow.keras import layers, models, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from google.colab import drive

# === TPU STRATEGY SETUP ===
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU:', tpu.master())
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
except:
    print('TPU not found. Using default strategy')
    strategy = tf.distribute.get_strategy()

# === MOUNT GOOGLE DRIVE ===
drive.mount('/content/drive')

# === PATHS ===
real_folder = '/content/drive/MyDrive/extracted_real_features_1000/'
fake_folder = '/content/drive/MyDrive/extracted_fake_features_1000/'
real_eda_path = os.path.join(real_folder, 'Real_EDA_Report_1000.xlsx')
fake_eda_path = os.path.join(fake_folder, 'Fake_EDA_Report_1000.xlsx')

# === LOAD FEATURES ===
def load_and_group_features(folder_path):
    audio_features = {}
    for file in os.listdir(folder_path):
        if file.endswith('.npy'):
            file_path = os.path.join(folder_path, file)
            audio_id = "_".join(file.split('_')[:3])
            if audio_id not in audio_features:
                audio_features[audio_id] = {}
            if 'mel_spec' in file:
                audio_features[audio_id]['mel_spectrogram'] = np.load(file_path)
            elif 'log_power_spec' in file:
                audio_features[audio_id]['log_spectrogram'] = np.load(file_path)
    return audio_features

real_audio_features = load_and_group_features(real_folder)
fake_audio_features = load_and_group_features(fake_folder)
audio_features = {**real_audio_features, **fake_audio_features}

# === NORMALIZE FEATURES ===
def normalize_feature(feature):
    scaler = MinMaxScaler()
    return scaler.fit_transform(feature.T).T if feature.ndim > 1 else scaler.fit_transform(feature.reshape(-1, 1)).reshape(feature.shape)

def normalize_all_features(audio_features):
    normalized_data = {}
    for audio_id, features in audio_features.items():
        if all(k in features for k in ['mel_spectrogram', 'log_spectrogram']):
            normalized_data[audio_id] = {k: normalize_feature(v) for k, v in features.items()}
    return normalized_data

normalized_data = normalize_all_features(audio_features)

# === COMBINE FEATURES ===
def match_sample_size(features):
    target_samples = min(map(lambda x: x.shape[0], features.values()))
    return {k: v[:target_samples] for k, v in features.items()}

def combine_features(features):
    return np.stack([features['mel_spectrogram'], features['log_spectrogram']], axis=-1)

combined = {aid: combine_features(match_sample_size(feats)) for aid, feats in normalized_data.items()}
audio_ids = list(combined.keys())

# === PAD TO MAX SHAPE ===
def pad_features(features_list, max_timesteps, max_freq_bins):
    padded_features = []
    for feature in features_list:
        pad_t = max(0, max_timesteps - feature.shape[0])
        pad_f = max(0, max_freq_bins - feature.shape[1])
        padded = np.pad(feature, ((0, pad_t), (0, pad_f), (0, 0)), mode='constant')
        padded_features.append(padded)
    return np.stack(padded_features)

max_timesteps = max(f.shape[0] for f in combined.values())
max_freq_bins = max(f.shape[1] for f in combined.values())
padded_data = pad_features(list(combined.values()), max_timesteps, max_freq_bins)

# === LOAD EDA LABELS ===
real_df = pd.read_excel(real_eda_path)
fake_df = pd.read_excel(fake_eda_path)
real_df['Label'] = 0
fake_df['Label'] = 1
eda_df = pd.concat([real_df, fake_df], ignore_index=True)
eda_df['Filename'] = eda_df['Filename'].str.replace('.flac', '', regex=False)
audio_id_to_label = dict(zip(eda_df['Filename'], eda_df['Label']))

# === MATCH LABELS ===
valid_audio_ids = [aid for aid in audio_ids if aid in audio_id_to_label]
X = np.stack([padded_data[audio_ids.index(aid)] for aid in valid_audio_ids])
y = np.array([audio_id_to_label[aid] for aid in valid_audio_ids])

# === DATA AUGMENTATION ===
def time_warp(signal, warp_factor=0.1):
    length = signal.shape[0]
    warp_size = int(length * warp_factor)
    src_points = np.linspace(0, length - 1, num=5)
    displacement = np.random.randint(-warp_size, warp_size + 1, size=5)
    dst_points = np.clip(src_points + displacement, 0, length - 1)
    return np.interp(np.arange(length), src_points, signal[dst_points.astype(int)])

def augment_data(X, y):
    augmented_X = []
    augmented_y = []
    for sample, label in zip(X, y):
        warped_channels = []
        for ch in range(sample.shape[-1]):
            warped = np.stack([time_warp(sample[:, f, ch]) for f in range(sample.shape[1])], axis=1)
            warped_channels.append(warped)
        warped = np.stack(warped_channels, axis=-1)
        augmented_X.append(warped)
        augmented_y.append(label)

        noisy = sample + 0.01 * np.random.normal(size=sample.shape)
        augmented_X.append(noisy)
        augmented_y.append(label)
    return np.concatenate([X, np.array(augmented_X)]), np.concatenate([y, np.array(augmented_y)])

X_augmented, y_augmented = augment_data(X, y)

# === TRAIN/TEST SPLIT ===
X_train, X_test, y_train, y_test = train_test_split(
    X_augmented, y_augmented, test_size=0.2, stratify=y_augmented, random_state=42
)

# === BUILD TPU COMPATIBLE MODEL ===
def build_improved_model_tpu(input_shape):
    inputs = Input(shape=input_shape)

    x = layers.Conv2D(64, (3, 3), padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling2D((2, 2))(x)

    x = layers.Conv2D(128, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling2D((2, 2))(x)

    x = layers.Conv2D(256, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    attention = layers.Conv2D(1, (1, 1), activation='sigmoid')(x)
    x = layers.multiply([x, attention])

    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)

    model = models.Model(inputs, outputs)
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
    return model

with strategy.scope():
    model = build_improved_model_tpu(X_train.shape[1:])
    model.summary()

# === TRAINING ===
class_weights = class_weight.compute_class_weight(
    'balanced', classes=np.unique(y_train), y=y_train
)
class_weights = dict(enumerate(class_weights))

callbacks = [
    EarlyStopping(monitor='val_auc', patience=15, mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),
    ModelCheckpoint('/content/drive/MyDrive/best_model.keras', save_best_only=True, monitor='val_auc', mode='max')
]

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=32,
    class_weight=class_weights,
    callbacks=callbacks
)

# === EVALUATION ===
test_loss, test_acc, test_auc = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test AUC: {test_auc:.4f}")

y_pred = (model.predict(X_test) > 0.5).astype(int)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=["Real", "Fake"]))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Real", "Fake"], yticklabels=["Real", "Fake"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.legend()
plt.show()